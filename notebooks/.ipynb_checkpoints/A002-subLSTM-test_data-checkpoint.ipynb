{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subLSTM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "\n",
    "sys.path.append('../models/')\n",
    "from subLSTM import *\n",
    "\n",
    "sys.path.append('../src/common/')\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder and initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 100\n",
    "target_size = 10\n",
    "training_steps = 1000\n",
    "batch_size = 50\n",
    "time_steps = 784\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input image placeholder\n",
    "X = tf.placeholder(tf.float32, [None, time_steps])\n",
    "#input label placeholder\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing rnn object\n",
    "rnn = subLSTM_cell(time_steps, hidden_layer_size, target_size)\n",
    "\n",
    "# Getting all outputs from rnn\n",
    "outputs = rnn.get_outputs()\n",
    "\n",
    "# Getting final output through indexing after reversing\n",
    "last_output = outputs[-1]\n",
    "\n",
    "# As rnn model output the final layer through Relu activation softmax is\n",
    "# used for final output\n",
    "output = tf.nn.softmax(last_output)\n",
    "\n",
    "# Computing the Cross Entropy loss\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(output))\n",
    "\n",
    "# Trainning with RMSProp Optimizer\n",
    "train_step = tf.train.RMSPropOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Calculation of correct prediction and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(output, 1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 784) (50, 10)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c01a4eef60a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         sess.run(opt, feed_dict={x: batch[0], \n\u001b[0m\u001b[1;32m     11\u001b[0m                                  y: batch[1]})\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(20000):\n",
    "        #batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        \n",
    "        print(batch[0].shape, batch[1].shape)\n",
    "                \n",
    "        sess.run(opt, feed_dict={x: batch[0], \n",
    "                                 y: batch[1]})\n",
    "\n",
    "        if i %10==0:\n",
    "            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n",
    "            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 115.127 Train Accuracy: 12.0 Test Accuracy: 10.28\n",
      "Iteration: 100 Loss: 114.869 Train Accuracy: 32.0 Test Accuracy: 29.47\n",
      "Iteration: 200 Loss: 110.978 Train Accuracy: 52.0 Test Accuracy: 31.5\n",
      "Iteration: 300 Loss: 108.515 Train Accuracy: 40.0 Test Accuracy: 35.94\n",
      "Iteration: 400 Loss: 103.137 Train Accuracy: 40.0 Test Accuracy: 37.97\n",
      "Iteration: 500 Loss: 96.1345 Train Accuracy: 44.0 Test Accuracy: 38.92\n",
      "Iteration: 600 Loss: 83.0607 Train Accuracy: 52.0 Test Accuracy: 39.64\n",
      "Iteration: 700 Loss: 85.9471 Train Accuracy: 48.0 Test Accuracy: 40.41\n",
      "Iteration: 800 Loss: 80.5137 Train Accuracy: 48.0 Test Accuracy: 41.85\n",
      "Iteration: 900 Loss: 88.2158 Train Accuracy: 40.0 Test Accuracy: 44.6\n",
      "Iteration: 1000 Loss: 67.1329 Train Accuracy: 50.0 Test Accuracy: 47.56\n",
      "Iteration: 1100 Loss: 75.3268 Train Accuracy: 54.0 Test Accuracy: 51.54\n",
      "Iteration: 1200 Loss: 73.0055 Train Accuracy: 58.0 Test Accuracy: 56.87\n",
      "Iteration: 1300 Loss: 63.4387 Train Accuracy: 60.0 Test Accuracy: 60.45\n",
      "Iteration: 1400 Loss: 79.1066 Train Accuracy: 54.0 Test Accuracy: 61.65\n",
      "Iteration: 1500 Loss: 59.9192 Train Accuracy: 70.0 Test Accuracy: 62.46\n",
      "Iteration: 1600 Loss: 59.5823 Train Accuracy: 72.0 Test Accuracy: 62.93\n",
      "Iteration: 1700 Loss: 64.9723 Train Accuracy: 66.0 Test Accuracy: 63.39\n",
      "Iteration: 1800 Loss: 64.6549 Train Accuracy: 66.0 Test Accuracy: 63.6\n",
      "Iteration: 1900 Loss: 60.7891 Train Accuracy: 60.0 Test Accuracy: 64.05\n",
      "Iteration: 2000 Loss: 43.3772 Train Accuracy: 74.0 Test Accuracy: 65.86\n",
      "Iteration: 2100 Loss: 37.8444 Train Accuracy: 82.0 Test Accuracy: 68.37\n",
      "Iteration: 2200 Loss: 45.2031 Train Accuracy: 74.0 Test Accuracy: 70.38\n",
      "Iteration: 2300 Loss: 65.7805 Train Accuracy: 56.0 Test Accuracy: 71.39\n",
      "Iteration: 2400 Loss: 43.4683 Train Accuracy: 76.0 Test Accuracy: 72.03\n",
      "Iteration: 2500 Loss: 45.1311 Train Accuracy: 74.0 Test Accuracy: 72.51\n",
      "Iteration: 2600 Loss: 40.5434 Train Accuracy: 74.0 Test Accuracy: 72.92\n",
      "Iteration: 2700 Loss: 41.2745 Train Accuracy: 78.0 Test Accuracy: 73.25\n",
      "Iteration: 2800 Loss: 50.649 Train Accuracy: 70.0 Test Accuracy: 73.43\n",
      "Iteration: 2900 Loss: 46.2091 Train Accuracy: 76.0 Test Accuracy: 73.45\n",
      "Iteration: 3000 Loss: 41.0027 Train Accuracy: 72.0 Test Accuracy: 73.58\n",
      "Iteration: 3100 Loss: 45.1477 Train Accuracy: 70.0 Test Accuracy: 73.74\n",
      "Iteration: 3200 Loss: 55.0964 Train Accuracy: 64.0 Test Accuracy: 73.82\n",
      "Iteration: 3300 Loss: 47.8086 Train Accuracy: 68.0 Test Accuracy: 73.9\n",
      "Iteration: 3400 Loss: 46.7565 Train Accuracy: 68.0 Test Accuracy: 73.99\n",
      "Iteration: 3500 Loss: 41.7192 Train Accuracy: 74.0 Test Accuracy: 74.13\n",
      "Iteration: 3600 Loss: 40.9213 Train Accuracy: 72.0 Test Accuracy: 74.15\n",
      "Iteration: 3700 Loss: 44.4308 Train Accuracy: 72.0 Test Accuracy: 74.19\n",
      "Iteration: 3800 Loss: 37.2756 Train Accuracy: 76.0 Test Accuracy: 74.3\n",
      "Iteration: 3900 Loss: 24.5498 Train Accuracy: 88.0 Test Accuracy: 74.27\n",
      "Iteration: 4000 Loss: 36.9611 Train Accuracy: 74.0 Test Accuracy: 74.33\n",
      "Iteration: 4100 Loss: 57.1433 Train Accuracy: 62.0 Test Accuracy: 74.38\n",
      "Iteration: 4200 Loss: 24.0203 Train Accuracy: 84.0 Test Accuracy: 74.41\n",
      "Iteration: 4300 Loss: 28.4182 Train Accuracy: 80.0 Test Accuracy: 74.52\n",
      "Iteration: 4400 Loss: 36.1371 Train Accuracy: 74.0 Test Accuracy: 74.51\n",
      "Iteration: 4500 Loss: 31.8122 Train Accuracy: 78.0 Test Accuracy: 74.56\n",
      "Iteration: 4600 Loss: 36.6 Train Accuracy: 76.0 Test Accuracy: 74.65\n",
      "Iteration: 4700 Loss: 36.4403 Train Accuracy: 76.0 Test Accuracy: 74.75\n",
      "Iteration: 4800 Loss: 50.3354 Train Accuracy: 66.0 Test Accuracy: 74.76\n",
      "Iteration: 4900 Loss: 38.8478 Train Accuracy: 70.0 Test Accuracy: 74.83\n",
      "Iteration: 5000 Loss: 27.2735 Train Accuracy: 84.0 Test Accuracy: 74.8\n",
      "Iteration: 5100 Loss: 53.7108 Train Accuracy: 68.0 Test Accuracy: 74.89\n",
      "Iteration: 5200 Loss: 39.2753 Train Accuracy: 74.0 Test Accuracy: 74.93\n",
      "Iteration: 5300 Loss: 42.5511 Train Accuracy: 72.0 Test Accuracy: 74.91\n",
      "Iteration: 5400 Loss: 49.8696 Train Accuracy: 68.0 Test Accuracy: 74.94\n",
      "Iteration: 5500 Loss: 27.1018 Train Accuracy: 82.0 Test Accuracy: 74.96\n",
      "Iteration: 5600 Loss: 32.1903 Train Accuracy: 76.0 Test Accuracy: 74.94\n",
      "Iteration: 5700 Loss: 42.1886 Train Accuracy: 72.0 Test Accuracy: 74.89\n",
      "Iteration: 5800 Loss: 35.4317 Train Accuracy: 74.0 Test Accuracy: 75.0\n",
      "Iteration: 5900 Loss: 32.3271 Train Accuracy: 78.0 Test Accuracy: 75.03\n",
      "Iteration: 6000 Loss: 37.098 Train Accuracy: 70.0 Test Accuracy: 75.11\n",
      "Iteration: 6100 Loss: 37.287 Train Accuracy: 72.0 Test Accuracy: 75.2\n",
      "Iteration: 6200 Loss: 26.6287 Train Accuracy: 82.0 Test Accuracy: 75.15\n",
      "Iteration: 6300 Loss: 46.0847 Train Accuracy: 66.0 Test Accuracy: 75.16\n",
      "Iteration: 6400 Loss: 43.4671 Train Accuracy: 72.0 Test Accuracy: 75.22\n",
      "Iteration: 6500 Loss: 26.6616 Train Accuracy: 82.0 Test Accuracy: 75.17\n",
      "Iteration: 6600 Loss: 38.5146 Train Accuracy: 74.0 Test Accuracy: 75.19\n",
      "Iteration: 6700 Loss: 32.7805 Train Accuracy: 74.0 Test Accuracy: 75.22\n",
      "Iteration: 6800 Loss: 39.7237 Train Accuracy: 70.0 Test Accuracy: 75.31\n",
      "Iteration: 6900 Loss: 44.5472 Train Accuracy: 74.0 Test Accuracy: 75.25\n",
      "Iteration: 7000 Loss: 34.0798 Train Accuracy: 78.0 Test Accuracy: 75.27\n",
      "Iteration: 7100 Loss: 38.2157 Train Accuracy: 74.0 Test Accuracy: 75.26\n",
      "Iteration: 7200 Loss: 29.9291 Train Accuracy: 80.0 Test Accuracy: 75.39\n",
      "Iteration: 7300 Loss: 45.0457 Train Accuracy: 66.0 Test Accuracy: 75.42\n",
      "Iteration: 7400 Loss: 45.4008 Train Accuracy: 68.0 Test Accuracy: 75.43\n",
      "Iteration: 7500 Loss: 38.321 Train Accuracy: 76.0 Test Accuracy: 75.47\n",
      "Iteration: 7600 Loss: 46.2536 Train Accuracy: 68.0 Test Accuracy: 75.47\n",
      "Iteration: 7700 Loss: 47.6354 Train Accuracy: 68.0 Test Accuracy: 75.43\n",
      "Iteration: 7800 Loss: 43.1376 Train Accuracy: 70.0 Test Accuracy: 75.49\n",
      "Iteration: 7900 Loss: 36.2041 Train Accuracy: 76.0 Test Accuracy: 75.46\n",
      "Iteration: 8000 Loss: 38.9861 Train Accuracy: 74.0 Test Accuracy: 75.48\n",
      "Iteration: 8100 Loss: 25.3979 Train Accuracy: 84.0 Test Accuracy: 75.47\n",
      "Iteration: 8200 Loss: 31.8084 Train Accuracy: 76.0 Test Accuracy: 75.46\n",
      "Iteration: 8300 Loss: 38.513 Train Accuracy: 70.0 Test Accuracy: 75.53\n",
      "Iteration: 8400 Loss: 21.5658 Train Accuracy: 88.0 Test Accuracy: 75.57\n",
      "Iteration: 8500 Loss: 26.6709 Train Accuracy: 84.0 Test Accuracy: 75.53\n",
      "Iteration: 8600 Loss: 26.6935 Train Accuracy: 84.0 Test Accuracy: 75.6\n",
      "Iteration: 8700 Loss: 25.2998 Train Accuracy: 84.0 Test Accuracy: 75.54\n",
      "Iteration: 8800 Loss: 34.6602 Train Accuracy: 74.0 Test Accuracy: 75.59\n",
      "Iteration: 8900 Loss: 38.0557 Train Accuracy: 70.0 Test Accuracy: 75.61\n",
      "Iteration: 9000 Loss: 23.6214 Train Accuracy: 84.0 Test Accuracy: 75.67\n",
      "Iteration: 9100 Loss: 48.4192 Train Accuracy: 70.0 Test Accuracy: 75.63\n",
      "Iteration: 9200 Loss: 31.6349 Train Accuracy: 78.0 Test Accuracy: 75.62\n",
      "Iteration: 9300 Loss: 37.9264 Train Accuracy: 72.0 Test Accuracy: 75.65\n",
      "Iteration: 9400 Loss: 25.0199 Train Accuracy: 80.0 Test Accuracy: 75.69\n",
      "Iteration: 9500 Loss: 33.5734 Train Accuracy: 76.0 Test Accuracy: 75.71\n",
      "Iteration: 9600 Loss: 29.7578 Train Accuracy: 78.0 Test Accuracy: 75.74\n",
      "Iteration: 9700 Loss: 35.8535 Train Accuracy: 70.0 Test Accuracy: 75.73\n",
      "Iteration: 9800 Loss: 13.3455 Train Accuracy: 92.0 Test Accuracy: 75.8\n",
      "Iteration: 9900 Loss: 30.0954 Train Accuracy: 76.0 Test Accuracy: 75.79\n",
      "Iteration: 10000 Loss: 34.9221 Train Accuracy: 76.0 Test Accuracy: 75.74\n",
      "Iteration: 10100 Loss: 45.477 Train Accuracy: 64.0 Test Accuracy: 75.69\n",
      "Iteration: 10200 Loss: 13.5237 Train Accuracy: 92.0 Test Accuracy: 75.83\n",
      "Iteration: 10300 Loss: 29.1307 Train Accuracy: 78.0 Test Accuracy: 75.78\n",
      "Iteration: 10400 Loss: 44.6193 Train Accuracy: 66.0 Test Accuracy: 75.85\n",
      "Iteration: 10500 Loss: 35.5115 Train Accuracy: 76.0 Test Accuracy: 75.86\n",
      "Iteration: 10600 Loss: 31.0729 Train Accuracy: 80.0 Test Accuracy: 75.83\n",
      "Iteration: 10700 Loss: 31.1995 Train Accuracy: 76.0 Test Accuracy: 75.89\n",
      "Iteration: 10800 Loss: 27.4418 Train Accuracy: 80.0 Test Accuracy: 75.86\n",
      "Iteration: 10900 Loss: 32.3528 Train Accuracy: 80.0 Test Accuracy: 75.86\n",
      "Iteration: 11000 Loss: 18.2038 Train Accuracy: 90.0 Test Accuracy: 75.9\n",
      "Iteration: 11100 Loss: 43.3277 Train Accuracy: 66.0 Test Accuracy: 75.89\n",
      "Iteration: 11200 Loss: 36.1657 Train Accuracy: 76.0 Test Accuracy: 75.9\n",
      "Iteration: 11300 Loss: 36.3853 Train Accuracy: 74.0 Test Accuracy: 75.91\n",
      "Iteration: 11400 Loss: 40.796 Train Accuracy: 68.0 Test Accuracy: 75.87\n",
      "Iteration: 11500 Loss: 24.7956 Train Accuracy: 82.0 Test Accuracy: 75.94\n",
      "Iteration: 11600 Loss: 42.031 Train Accuracy: 68.0 Test Accuracy: 75.87\n",
      "Iteration: 11700 Loss: 31.5423 Train Accuracy: 78.0 Test Accuracy: 75.95\n",
      "Iteration: 11800 Loss: 36.2218 Train Accuracy: 72.0 Test Accuracy: 75.9\n",
      "Iteration: 11900 Loss: 28.0868 Train Accuracy: 80.0 Test Accuracy: 75.97\n",
      "Iteration: 12000 Loss: 31.6225 Train Accuracy: 76.0 Test Accuracy: 75.95\n",
      "Iteration: 12100 Loss: 31.8008 Train Accuracy: 78.0 Test Accuracy: 76.03\n",
      "Iteration: 12200 Loss: 14.9147 Train Accuracy: 92.0 Test Accuracy: 75.97\n",
      "Iteration: 12300 Loss: 43.3566 Train Accuracy: 70.0 Test Accuracy: 75.99\n",
      "Iteration: 12400 Loss: 25.6526 Train Accuracy: 80.0 Test Accuracy: 75.96\n",
      "Iteration: 12500 Loss: 29.1499 Train Accuracy: 82.0 Test Accuracy: 76.01\n",
      "Iteration: 12600 Loss: 27.6106 Train Accuracy: 80.0 Test Accuracy: 75.96\n",
      "Iteration: 12700 Loss: 37.7999 Train Accuracy: 70.0 Test Accuracy: 76.02\n",
      "Iteration: 12800 Loss: 27.5796 Train Accuracy: 78.0 Test Accuracy: 75.99\n",
      "Iteration: 12900 Loss: 43.4319 Train Accuracy: 70.0 Test Accuracy: 76.01\n",
      "Iteration: 13000 Loss: 26.487 Train Accuracy: 82.0 Test Accuracy: 76.06\n",
      "Iteration: 13100 Loss: 28.7365 Train Accuracy: 76.0 Test Accuracy: 76.07\n",
      "Iteration: 13200 Loss: 41.9036 Train Accuracy: 66.0 Test Accuracy: 76.06\n",
      "Iteration: 13300 Loss: 28.5379 Train Accuracy: 80.0 Test Accuracy: 76.09\n",
      "Iteration: 13400 Loss: 35.1981 Train Accuracy: 70.0 Test Accuracy: 76.09\n",
      "Iteration: 13500 Loss: 42.182 Train Accuracy: 66.0 Test Accuracy: 76.07\n",
      "Iteration: 13600 Loss: 30.591 Train Accuracy: 78.0 Test Accuracy: 76.09\n",
      "Iteration: 13700 Loss: 16.5196 Train Accuracy: 88.0 Test Accuracy: 76.11\n",
      "Iteration: 13800 Loss: 31.9773 Train Accuracy: 74.0 Test Accuracy: 76.17\n",
      "Iteration: 13900 Loss: 31.8156 Train Accuracy: 78.0 Test Accuracy: 76.16\n",
      "Iteration: 14000 Loss: 29.0422 Train Accuracy: 78.0 Test Accuracy: 76.15\n",
      "Iteration: 14100 Loss: 15.8033 Train Accuracy: 88.0 Test Accuracy: 76.13\n",
      "Iteration: 14200 Loss: 41.1592 Train Accuracy: 72.0 Test Accuracy: 76.14\n",
      "Iteration: 14300 Loss: 36.9876 Train Accuracy: 72.0 Test Accuracy: 76.17\n",
      "Iteration: 14400 Loss: 52.6819 Train Accuracy: 60.0 Test Accuracy: 76.14\n",
      "Iteration: 14500 Loss: 22.6158 Train Accuracy: 84.0 Test Accuracy: 76.15\n",
      "Iteration: 14600 Loss: 32.1028 Train Accuracy: 78.0 Test Accuracy: 76.15\n",
      "Iteration: 14700 Loss: 28.3804 Train Accuracy: 76.0 Test Accuracy: 76.21\n",
      "Iteration: 14800 Loss: 26.7921 Train Accuracy: 82.0 Test Accuracy: 76.23\n",
      "Iteration: 14900 Loss: 31.5475 Train Accuracy: 76.0 Test Accuracy: 76.29\n",
      "Iteration: 15000 Loss: 18.8461 Train Accuracy: 86.0 Test Accuracy: 76.27\n",
      "Iteration: 15100 Loss: 28.5869 Train Accuracy: 78.0 Test Accuracy: 76.2\n",
      "Iteration: 15200 Loss: 27.8267 Train Accuracy: 80.0 Test Accuracy: 76.23\n",
      "Iteration: 15300 Loss: 40.1337 Train Accuracy: 70.0 Test Accuracy: 76.2\n",
      "Iteration: 15400 Loss: 26.9055 Train Accuracy: 82.0 Test Accuracy: 76.29\n",
      "Iteration: 15500 Loss: 26.4112 Train Accuracy: 84.0 Test Accuracy: 76.25\n",
      "Iteration: 15600 Loss: 21.9359 Train Accuracy: 84.0 Test Accuracy: 76.25\n",
      "Iteration: 15700 Loss: 41.4442 Train Accuracy: 68.0 Test Accuracy: 76.31\n",
      "Iteration: 15800 Loss: 38.8841 Train Accuracy: 70.0 Test Accuracy: 76.32\n",
      "Iteration: 15900 Loss: 33.524 Train Accuracy: 74.0 Test Accuracy: 76.31\n",
      "Iteration: 16000 Loss: 35.2311 Train Accuracy: 72.0 Test Accuracy: 76.26\n",
      "Iteration: 16100 Loss: 37.57 Train Accuracy: 74.0 Test Accuracy: 76.3\n",
      "Iteration: 16200 Loss: 14.7723 Train Accuracy: 90.0 Test Accuracy: 76.33\n",
      "Iteration: 16300 Loss: 26.2763 Train Accuracy: 82.0 Test Accuracy: 76.4\n",
      "Iteration: 16400 Loss: 39.1069 Train Accuracy: 70.0 Test Accuracy: 76.3\n",
      "Iteration: 16500 Loss: 22.421 Train Accuracy: 88.0 Test Accuracy: 76.35\n",
      "Iteration: 16600 Loss: 26.6712 Train Accuracy: 84.0 Test Accuracy: 76.32\n",
      "Iteration: 16700 Loss: 30.8172 Train Accuracy: 74.0 Test Accuracy: 76.37\n",
      "Iteration: 16800 Loss: 29.8514 Train Accuracy: 76.0 Test Accuracy: 76.29\n",
      "Iteration: 16900 Loss: 30.8438 Train Accuracy: 76.0 Test Accuracy: 76.41\n",
      "Iteration: 17000 Loss: 35.7707 Train Accuracy: 76.0 Test Accuracy: 76.45\n",
      "Iteration: 17100 Loss: 28.0133 Train Accuracy: 78.0 Test Accuracy: 76.44\n",
      "Iteration: 17200 Loss: 38.2359 Train Accuracy: 70.0 Test Accuracy: 76.44\n",
      "Iteration: 17300 Loss: 26.3115 Train Accuracy: 82.0 Test Accuracy: 76.43\n",
      "Iteration: 17400 Loss: 29.5382 Train Accuracy: 80.0 Test Accuracy: 76.37\n",
      "Iteration: 17500 Loss: 45.609 Train Accuracy: 68.0 Test Accuracy: 76.4\n",
      "Iteration: 17600 Loss: 31.7359 Train Accuracy: 80.0 Test Accuracy: 76.42\n",
      "Iteration: 17700 Loss: 27.7972 Train Accuracy: 78.0 Test Accuracy: 76.46\n",
      "Iteration: 17800 Loss: 30.1636 Train Accuracy: 76.0 Test Accuracy: 76.42\n",
      "Iteration: 17900 Loss: 37.3143 Train Accuracy: 70.0 Test Accuracy: 76.44\n",
      "Iteration: 18000 Loss: 35.3536 Train Accuracy: 74.0 Test Accuracy: 76.46\n",
      "Iteration: 18100 Loss: 20.6434 Train Accuracy: 84.0 Test Accuracy: 76.49\n",
      "Iteration: 18200 Loss: 34.5158 Train Accuracy: 76.0 Test Accuracy: 76.48\n",
      "Iteration: 18300 Loss: 29.3378 Train Accuracy: 76.0 Test Accuracy: 76.46\n",
      "Iteration: 18400 Loss: 25.6438 Train Accuracy: 84.0 Test Accuracy: 76.5\n",
      "Iteration: 18500 Loss: 29.2555 Train Accuracy: 76.0 Test Accuracy: 76.48\n",
      "Iteration: 18600 Loss: 32.6808 Train Accuracy: 78.0 Test Accuracy: 76.54\n",
      "Iteration: 18700 Loss: 26.1414 Train Accuracy: 82.0 Test Accuracy: 76.49\n",
      "Iteration: 18800 Loss: 26.6934 Train Accuracy: 80.0 Test Accuracy: 76.5\n",
      "Iteration: 18900 Loss: 31.4306 Train Accuracy: 76.0 Test Accuracy: 76.47\n",
      "Iteration: 19000 Loss: 35.9848 Train Accuracy: 74.0 Test Accuracy: 76.55\n",
      "Iteration: 19100 Loss: 35.9036 Train Accuracy: 76.0 Test Accuracy: 76.53\n",
      "Iteration: 19200 Loss: 19.6816 Train Accuracy: 86.0 Test Accuracy: 76.57\n",
      "Iteration: 19300 Loss: 34.7041 Train Accuracy: 74.0 Test Accuracy: 76.54\n",
      "Iteration: 19400 Loss: 18.8347 Train Accuracy: 84.0 Test Accuracy: 76.59\n",
      "Iteration: 19500 Loss: 25.3815 Train Accuracy: 82.0 Test Accuracy: 76.57\n",
      "Iteration: 19600 Loss: 32.7624 Train Accuracy: 76.0 Test Accuracy: 76.62\n",
      "Iteration: 19700 Loss: 30.4804 Train Accuracy: 76.0 Test Accuracy: 76.64\n",
      "Iteration: 19800 Loss: 19.2268 Train Accuracy: 86.0 Test Accuracy: 76.59\n",
      "Iteration: 19900 Loss: 22.2129 Train Accuracy: 88.0 Test Accuracy: 76.62\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_epoch = 20000\n",
    "losses = np.empty(n_epoch)\n",
    "train_accuracies = np.empty(n_epoch)\n",
    "test_accuracies = np.empty(n_epoch)\n",
    "\n",
    "#Iterations to do trainning\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    X, Y = mnist.train.next_batch(batch_size)\n",
    "    X = X.reshape(batch_size, 1, 784)\n",
    "    sess.run(train_step,feed_dict={rnn._inputs:X, y:Y})\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        Loss=str(sess.run(cross_entropy,feed_dict={rnn._inputs:X, y:Y}))\n",
    "        Train_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X, y:Y}))\n",
    "        X_test = mnist.test.images.reshape(10000,1,784)\n",
    "        Test_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X_test, y:mnist.test.labels}))\n",
    "    \n",
    "        losses[epoch] = Loss\n",
    "        train_accuracies[epoch] = Train_accuracy\n",
    "        test_accuracies[epoch] = Test_accuracy\n",
    "    \n",
    "        print(\"\\rIteration: %s Loss: %s Train Accuracy: %s Test Accuracy: %s\"%(epoch,Loss,Train_accuracy,Test_accuracy)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-97dc1af2996c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_epoch = 1020\n",
    "losses = np.empty(n_epoch)\n",
    "train_accuracies = np.empty(n_epoch)\n",
    "test_accuracies = np.empty(n_epoch)\n",
    "\n",
    "#Iterations to do trainning\n",
    "for epoch in range(n_epoch):\n",
    "    start=0\n",
    "    end=100\n",
    "    for i in range(14):\n",
    "        \n",
    "        \n",
    "        \n",
    "        X=X_train[start:end]\n",
    "        Y=y_train[start:end]\n",
    "        start=end\n",
    "        end=start+100\n",
    "        sess.run(train_step,feed_dict={rnn._inputs:X, y:Y})\n",
    "\n",
    "    Loss=str(sess.run(cross_entropy,feed_dict={rnn._inputs:X, y:Y}))\n",
    "    Train_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X_train[:500], y:y_train[:500]}))\n",
    "    Test_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X_test, y:y_test}))\n",
    "    \n",
    "    losses[epoch] = Loss\n",
    "    train_accuracies[epoch] = Train_accuracy\n",
    "    test_accuracies[epoch] = Test_accuracy\n",
    "    \n",
    "    print(\"\\rIteration: %s Loss: %s Train Accuracy: %s Test Accuracy: %s\"%(epoch,Loss,Train_accuracy,Test_accuracy)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12698ad68>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQTgqKbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12446fa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(n_epoch),losses,'b.',)\n",
    "plt.plot(range(n_epoch),train_accuracies,'r*',)\n",
    "plt.plot(range(n_epoch),test_accuracies,'g+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
